{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 19:28:55.998801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 19:28:56.449746: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:28:56.449810: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-21 19:28:57.583736: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:28:57.583864: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:28:57.583882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3469 Validation Data loaded\n"
     ]
    }
   ],
   "source": [
    "# load all test data\n",
    "x_test,y_test = load_pkl_data(test_pkl)\n",
    "print(\"%d Validation Data loaded\"%(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels loaded\n",
      "Total 3 classes\n"
     ]
    }
   ],
   "source": [
    "# load label maps\n",
    "label2ind = load_dict_from_json(label2ind_json)\n",
    "ind2label = load_dict_from_json(ind2label_json)\n",
    "print(\"Labels loaded\")\n",
    "nc = len(label2ind.keys())\n",
    "print(\"Total %d classes\"%nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 19:29:00.051651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-21 19:29:00.052102: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.052450: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.052725: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.052994: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.053216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.053337: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.053621: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.053723: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-21 19:29:00.053742: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-21 19:29:00.054129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created\n"
     ]
    }
   ],
   "source": [
    "model = AttentionClassifier(nc)\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored weights from model/ckpt-10\n"
     ]
    }
   ],
   "source": [
    "model.load_model(\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Index 3169\n",
      "danieebro all these bitches fake if you ask me\n",
      "Label: offensive_language\n"
     ]
    }
   ],
   "source": [
    "# take a random data from test\n",
    "n_test = len(x_test)\n",
    "rand_idx = np.random.randint(0,n_test)\n",
    "print(\"Random Index %d\"%rand_idx)\n",
    "test_tweet = x_test[rand_idx]\n",
    "test_label = y_test[rand_idx]\n",
    "print(test_tweet)\n",
    "print(\"Label: %s\"%test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 0.26615798473358154\n",
      "danieebro 0.14215525425970554\n",
      "all 0.021965084597468376\n",
      "these 0.004194268025457859\n",
      "bitches 0.04056389629840851\n",
      "fake 0.03019515983760357\n",
      "if 0.07169847190380096\n",
      "you 0.008941817097365856\n",
      "ask 0.12221364676952362\n",
      "me 0.003794984659180045\n",
      "[SEP] 0.28811943531036377\n"
     ]
    }
   ],
   "source": [
    "word_attention = model.predict_with_explain(test_tweet,ind2label)\n",
    "for w in word_attention:\n",
    "    print(w[0],w[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] angelis yep which is messier more dangerous more environmentally unfriendly than their retarded new spouts [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded = model.encoder_layer.preprocess_layer.decode(tf.squeeze(words,-1)) \n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ['[CLS]']\n",
      "[1, 2] ['angel', '##is']\n",
      "[3] ['yep']\n",
      "[4] ['which']\n",
      "[5] ['is']\n",
      "[6, 7] ['mess', '##ier']\n",
      "[8] ['more']\n",
      "[9] ['dangerous']\n",
      "[10] ['more']\n",
      "[11] ['environmentally']\n",
      "[12, 13, 14, 15] ['un', '##fr', '##ien', '##dly']\n",
      "[16] ['than']\n",
      "[17] ['their']\n",
      "[18, 19, 20] ['re', '##tar', '##ded']\n",
      "[21] ['new']\n",
      "[22, 23] ['sp', '##outs']\n",
      "[24] ['[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sub_word_ids = model.encoder_layer.preprocess_layer.encode(test_tweet)\n",
    "sub_word_dict = {}\n",
    "for i,sw in enumerate(sub_word_ids):\n",
    "    sub_word_dict[i] = sw\n",
    "# print(sub_word_dict)\n",
    "word_spans = []\n",
    "subword_list = []\n",
    "for k in sub_word_dict.keys():\n",
    "    sid = sub_word_dict[k]\n",
    "    sub_word = model.encoder_layer.preprocess_layer.convert_ids_to_tokens(sid)\n",
    "    if \"##\" in sub_word:\n",
    "        word_spans[-1].append(k)\n",
    "        subword_list[-1].append(sub_word)\n",
    "    else:\n",
    "        word_spans.append([k])\n",
    "        subword_list.append([sub_word])\n",
    "    # print(k,sid,sub_word)\n",
    "for i,s in zip(word_spans,subword_list):\n",
    "    print(i,s)\n",
    "# all_spans = []\n",
    "# for id_ in word_ids:\n",
    "#     sub_word = model.encoder_layer.preprocess_layer.convert_ids_to_tokens(id_)\n",
    "#     if \"##\" in sub_word:\n",
    "#         all_spans[-1].append(id_)\n",
    "#     else:\n",
    "#         all_spans.append([id_])\n",
    "#     # print(id_,sub_word)\n",
    "# word_span = {}\n",
    "# for i,sp in enumerate(all_spans):\n",
    "#     sub_list = []\n",
    "#     sub_idx = []\n",
    "#     for s in sp:\n",
    "#         sub_word = model.encoder_layer.preprocess_layer.convert_ids_to_tokens(s)\n",
    "#         sub_list.append(sub_word.replace(\"##\",\"\"))\n",
    "#         sub_idx.append(s)\n",
    "#     wd = ''.join(sub_list)\n",
    "#     word_span[str(i)] = {\"word\":wd,\"subword_index\":sub_idx}\n",
    "# print(word_span)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatras_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
